{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd60f5b",
   "metadata": {},
   "source": [
    "## search_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c581f6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 쿼리를 입력하세요.My name is Sabin\n",
      "rank\tIndex\tscore\tsentence\n",
      "1\t679\t0.6\tmy name is mike.\n",
      "2\t526\t0.3333333333333333\tbob is my brother.\n",
      "3\t538\t0.3333333333333333\tmy hobby is traveling.\n",
      "4\t453\t0.2857142857142857\tmy mother is sketching them.\n",
      "5\t241\t0.25\tmy father is running with so-ra.\n",
      "6\t336\t0.25\tmy family is at the park.\n",
      "7\t212\t0.2222222222222222\tmy sister betty is waiting for me.\n",
      "8\t505\t0.2\tmy little sister annie is five years old.\n",
      "9\t190\t0.16666666666666666\tit is sunday.\n",
      "10\t314\t0.16666666666666666\tthis is washington.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def lower_case(early_sentence) : #입력 : 초기 검색 쿼리 or 초기 검색 대상의 각 문장\n",
    "    lower_sentence = ''.join(early_sentence).lower()\n",
    "    return lower_sentence\n",
    "    #반환 : 소문자화된 검색 쿼리 or 문장\n",
    "\n",
    "def preprocess(sentence) : #입력 : 소문자화된 검색 쿼리 or 검색 대상의 각 문장\n",
    "    preprocessed_sentence = sentence.strip().split(\" \") #토큰화 : 공백을 기준으로 문장 분할\n",
    "    return preprocessed_sentence \n",
    "    #반환 : 토큰화된 검색 쿼리 or 문장 (=쿼리와 문장 내 토큰 집합)\n",
    "    #쿼리는 바로 여기로 들어와서 토큰화해서 나가기\n",
    "    \n",
    "def indexing(file_name) : #입력 : 검색 대상 파일의 이름, 경로\n",
    "    file_tokens_pairs = [] #반환할 토큰 리스트 생성\n",
    "    for line in lines : #파일에 들어있는 문장 수만큼 반복문 실행\n",
    "        lower_line = lower_case(line) #소문자화하는 함수 호출\n",
    "        tokens = preprocess(lower_line) #토큰화하는 함수 호출\n",
    "        file_tokens_pairs.append(tokens) #전처리된 문장을 토큰 리스트에 추가\n",
    "    return file_tokens_pairs\n",
    "    #반환 : 파일 내 각 문장에 대한 토큰 리스트\n",
    "    #search target은 여기서 반복문 실행을 통해 한 문장씩 인덱싱하고 바깥에서 토큰화해서 다시 들어와서 저장됨\n",
    "    \n",
    "def calc_similarity(preprocessed_query, preprocessed_sentences) :\n",
    "    #입력 : 전처리된 검색 쿼리와 전처리된 검색 대상 문장\n",
    "    score_dict = {} #유사도 점수 저장할 딕셔너리 생성\n",
    "    for i in range(len(file_tokens_pairs)): #파일 내에 있는 문장 수 만큼 반복문 실행 \n",
    "        file_token_set = set(file_tokens_pairs[i]) #파일 내 각 문장에 대한 토큰 리스트를 집합으로 변경\n",
    "        all_tokens = query_token_set | file_token_set #all_tokens = 두 집합의 합집합\n",
    "        same_tokens = query_token_set & file_token_set #same_tokens = 두 집합의 교집합 \n",
    "        similarity = len(same_tokens) / len(all_tokens) #유사도 계산하기 (유사도=같은 토큰 수/전체 토큰 수)\n",
    "        score_dict[i] = similarity #계산한 유사도를 딕셔너리에 담기\n",
    "    return score_dict\n",
    "    #반환 : 파일 인덱스 및 쿼리와 문장의 유사도 점수를 가지는 dictionary\n",
    "\n",
    "\n",
    "'''순서\n",
    "#데이터 소스 -> preprocessing -> indexing -> index -> 유사도 계산\n",
    "#쿼리 입력받기 -> preprocessing -> preprocessed query -> 유사도 계산\n",
    "# => 순차정렬 -> 상위 10개 출력\n",
    "'''  \n",
    "    \n",
    "# 1. Indexing\n",
    "file_name = \"jhe-koen-dev.en\" #search target이 될 data source\n",
    "lines = open(file_name, \"r\", encoding=\"utf8\").readlines() #파일을 읽기 전용으로 열기\n",
    "file_tokens_pairs = indexing(file_name)\n",
    "\n",
    "    \n",
    "# 2. Input the query\n",
    "query = input(\"영어 쿼리를 입력하세요.\") #영어 쿼리 입력받기\n",
    "lower_query = lower_case(query)\n",
    "preprocessed_query = preprocess(lower_query) #함수 호출->입력받은 영어 쿼리 토큰화\n",
    "query_token_set = set(preprocessed_query) #토큰화한 영어 쿼리를 집합에 담기\n",
    "\n",
    "\n",
    "# 3. Calculate similarities based on a same token set\n",
    "score_dict = calc_similarity(query_token_set, file_tokens_pairs)\n",
    "#유사도 점수 저장된 딕셔너리 반환\n",
    "    \n",
    "# 4. Sort the similarity list\n",
    "sorted_score_list = sorted(score_dict.items(), key = operator.itemgetter(1), reverse=True)\n",
    "#딕셔너리 점수대로 순차 정렬\n",
    "\n",
    "\n",
    "# 5. Print the result\n",
    "if sorted_score_list[0][1] == 0.0:\n",
    "    print(\"There is no similar sentence.\")\n",
    "else:\n",
    "    print(\"rank\", \"Index\", \"score\", \"sentence\", sep = \"\\t\")\n",
    "    rank = 1 #rank 번호는 1부터 시작\n",
    "    for i, score  in sorted_score_list:\n",
    "        print(rank, i, score, ' '.join(file_tokens_pairs[i]), sep = \"\\t\")\n",
    "        if rank == 10:#상위 10개 문장 출력이므로 10개를 출력하면 반복문 종료\n",
    "            break\n",
    "        rank = rank + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
